# ðŸ‘‹ Hello, I'm ZHANG Erli!

## ðŸ‘¤ About me
I am a first-year PhD student at the National University of Singapore ðŸ‡¸ðŸ‡¬, majoring in Biomedical Engineering. Prior to this, I obtained a Bachelor of Engineering in Computer Science from Nanyang Technological University. My current research interests include **AI in Healthcare**, **Surgical Video Analysis**, and **Large Multimodal Models**.

- **Resume:** [Resume](https://github.com/ZhangErliCarl/ZhangErliCarl/blob/main/Zhang%20Erli%20CV%202024%20Dec.pdf)
- **Homepage:** [Homepage](https://zhangerlicarl.github.io/)
- **Google Scholar:** [Profile](https://scholar.google.com/citations?user=gfjYZKMAAAAJ&hl=en-US)
- **LinkedIn:** [LinkedIn](https://www.linkedin.com/in/zhang-erli/)

## ðŸ“– Publications

### [Surgical SAM 2: Real-time Segment Anything in Surgical Video by Efficient Frame Pruning](https://github.com/jinlab-imvr/Surgical-SAM-2)
- **Conference:** NeurIPS 2024 Workshop
- **Description:** Efficient Segment Anything 2 (SAM2) with frame pruning mechanism for real-time surgical video segmentation
- ðŸ“– [Paper](https://arxiv.org/pdf/2408.07931)

### [Q-Instruct: Improving Low-level Visual Abilities for Multi-modality Foundation Models](https://github.com/Q-Future/Q-Instruct)
- **Conference:** CVPR 2024
- **Description:** Low-level visual instruction tuning for multi-modality LLMs
- ðŸ“– [Paper](https://openaccess.thecvf.com/content/CVPR2024/papers/Wu_Q-Instruct_Improving_Low-level_Visual_Abilities_for_Multi-modality_Foundation_Models_CVPR_2024_paper.pdf)

### [Q-Bench: Multi-Modality Benchmarking](https://github.com/Q-Future/Q-Bench)
- **Conference:** ICLR 2024 (spotlight)
- **Description:** A benchmark for multi-modality LLMs on low-level vision and visual quality assessment.
- ðŸ“– [Paper](https://openreview.net/pdf?id=0V5TVt9bk0)

### [MaxVQA/MaxWell: Towards Explainable VQA](https://github.com/VQAssessment/MaxVQA)
- **Conference:** ACMMM 2023 (oral)
- **Description:** Introduced a 16-dimensional VQA Dataset and Method for a more explainable VQA.
- ðŸ“– [Paper](https://dl.acm.org/doi/pdf/10.1145/3581783.3611737)

### [DOVER: NR-VQA Method](https://github.com/VQAssessment/DOVER)
- **Conference:** ICCV 2023
- **Description:** A state-of-the-art NR-VQA method that predicts disentangled aesthetic and technical quality.
- ðŸ“– [Paper](https://openaccess.thecvf.com/content/ICCV2023/papers/Wu_Exploring_Video_Quality_Assessment_on_User_Generated_Contents_from_Aesthetic_ICCV_2023_paper.pdf)

## ðŸ“¬ Contact Me
- **Email:** zhangerlicarl@gmail.com or erli.zhang@nus.edu.sg
- **Twitter:** [@zhang_erli](https://twitter.com/zhang_erli)
